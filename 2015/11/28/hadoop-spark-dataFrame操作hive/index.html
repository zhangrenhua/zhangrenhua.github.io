<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="google-site-verification" content="zUNgN8yGBtBZ8194JTl7PXh7YvztVw494zGKJ_8qH00">








  <meta name="baidu-site-verification" content="kqLgluU8lc">







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico?v=5.1.4">


  <link rel="mask-icon" href="/favicon.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="java,hadoop生态圈,">





  <link rel="alternate" href="/atom.xml" title="hua的博客" type="application/atom+xml">






<meta name="description" content="从spark1.2 到spark1.3，spark SQL中的SchemaRDD变为了DataFrame，DataFrame相对于SchemaRDD有了较大改变，同时提供了更多好用且方便的API。本文主要演示如何在spark1.5.2中使用DataFrame将数据写入hive中以及DataFrame的一些其他API，仅供参考。">
<meta name="keywords" content="java,hadoop生态圈">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark DataFrme操作Hive">
<meta property="og:url" content="http://www.zhangrenhua.com/2015/11/28/hadoop-spark-dataFrame操作hive/index.html">
<meta property="og:site_name" content="hua的博客">
<meta property="og:description" content="从spark1.2 到spark1.3，spark SQL中的SchemaRDD变为了DataFrame，DataFrame相对于SchemaRDD有了较大改变，同时提供了更多好用且方便的API。本文主要演示如何在spark1.5.2中使用DataFrame将数据写入hive中以及DataFrame的一些其他API，仅供参考。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://www.zhangrenhua.com/back_images/spark-hive-insert-query.png">
<meta property="og:updated_time" content="2019-07-11T06:43:13.732Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark DataFrme操作Hive">
<meta name="twitter:description" content="从spark1.2 到spark1.3，spark SQL中的SchemaRDD变为了DataFrame，DataFrame相对于SchemaRDD有了较大改变，同时提供了更多好用且方便的API。本文主要演示如何在spark1.5.2中使用DataFrame将数据写入hive中以及DataFrame的一些其他API，仅供参考。">
<meta name="twitter:image" content="http://www.zhangrenhua.com/back_images/spark-hive-insert-query.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.zhangrenhua.com/2015/11/28/hadoop-spark-dataFrame操作hive/">





  <title>Spark DataFrme操作Hive | hua的博客</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-69647002-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?954f429681a4cd6dee6095ddaa2aac9a";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">hua的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">关注技术和人文的原创IT博客</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            日程表
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input">
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'BAacTgC1f_bLy6kngppZ','2.0.0');
</script>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.zhangrenhua.com/2015/11/28/hadoop-spark-dataFrame操作hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张仁华">
      <meta itemprop="description" content>
      <meta itemprop="image" content="http://www.zhangrenhua.com/back_images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hua的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Spark DataFrme操作Hive</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2015-11-28T20:10:00+08:00">
                2015-11-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2015/11/28/hadoop-spark-dataFrame操作hive/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2015/11/28/hadoop-spark-dataFrame操作hive/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          
              <div class="post-description">
                  从spark1.2 到spark1.3，spark SQL中的SchemaRDD变为了DataFrame，DataFrame相对于SchemaRDD有了较大改变，同时提供了更多好用且方便的API。<br>本文主要演示如何在spark1.5.2中使用DataFrame将数据写入hive中以及DataFrame的一些其他API，仅供参考。
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>从spark1.3起，spark SQL中的SchemaRDD变为了DataFrame，DataFrame相对于SchemaRDD有了较大改变，同时提供了更多好用且方便的API。</p>
<p>本文主要演示如何在spark1.5.2中使用DataFrame将数据写入hive中以及DataFrame的一些其他API，仅供参考。</p>
<h2 id="DataFrame-SaveAsTable"><a href="#DataFrame-SaveAsTable" class="headerlink" title="DataFrame SaveAsTable"></a>DataFrame SaveAsTable</h2><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>新建数据文件test.txt，并上次至HDFS的/test/zhangrenhua目录下:</p>
<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">zrh,</span><span class="number">19</span></span><br><span class="line"><span class="built_in">z,</span><span class="number">20</span></span><br><span class="line"><span class="built_in">r,</span><span class="number">21</span></span><br><span class="line"><span class="built_in">h,</span><span class="number">90</span></span><br></pre></td></tr></table></figure>

<p>创建hive表：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="keyword">test</span>;</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span></span><br><span class="line">    test.test;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span></span><br><span class="line">    test.test</span><br><span class="line">    (</span><br><span class="line">        t_string <span class="keyword">string</span>,</span><br><span class="line">        t_integer <span class="built_in">INT</span>,</span><br><span class="line">        t_boolean <span class="built_in">BOOLEAN</span>,</span><br><span class="line">        t_double <span class="keyword">DOUBLE</span>,</span><br><span class="line">        t_decimal <span class="built_in">DECIMAL</span>(<span class="number">20</span>,<span class="number">8</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">stored</span> <span class="keyword">AS</span> orc;</span><br></pre></td></tr></table></figure>

<p>Demo.java：</p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><span class="meta-keyword">import</span> java.math.BigDecimal;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> java.util.ArrayList;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> java.util.List;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.SparkConf;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.SparkContext;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.api.java.JavaRDD;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.api.java.function.Function;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.sql.DataFrame;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.sql.Row;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.sql.SaveMode;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.sql.hive.HiveContext;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.sql.types.DataTypes;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.sql.types.Decimal;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.sql.types.StructField;</span></span><br><span class="line"><span class="meta"><span class="meta-keyword">import</span> org.apache.spark.sql.types.StructType;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * spark程序入口</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Demo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">        String input = <span class="string">"/test/zhangrenhua"</span>;</span><br><span class="line">        SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"Demo"</span>);</span><br><span class="line">        SparkContext sc = <span class="keyword">new</span> SparkContext(sparkConf);</span><br><span class="line">        HiveContext hiveContext = <span class="keyword">new</span> HiveContext(sc);</span><br><span class="line">        hiveContext.setConf(<span class="string">"spark.sql.hive.metastore.version"</span>, <span class="string">"0.13.0.2.1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建spark Context</span></span><br><span class="line">        <span class="keyword">try</span> (JavaSparkContext ctx = <span class="keyword">new</span> JavaSparkContext(sc);) &#123;</span><br><span class="line">            <span class="comment">// 读取测试文件</span></span><br><span class="line">            JavaRDD&lt;String&gt; textFile = ctx.textFile(input);</span><br><span class="line">            JavaRDD&lt;Row&gt; map = textFile.map(<span class="keyword">new</span> Function&lt;String, Row&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                <span class="comment">/**</span></span><br><span class="line"><span class="comment">                 * </span></span><br><span class="line"><span class="comment">                 */</span></span><br><span class="line">                <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> long serialVersionUID = <span class="number">8745604304589989962</span>L;</span><br><span class="line"></span><br><span class="line">                @Override</span><br><span class="line">                <span class="keyword">public</span> Row call(String v1) throws Exception &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 解析测试数据，并setting默认值</span></span><br><span class="line">                    String[] array = v1.split(<span class="string">","</span>);</span><br><span class="line">                    TestBean result = <span class="keyword">new</span> TestBean();</span><br><span class="line">                    result.setT_string(array[<span class="number">0</span>]);</span><br><span class="line">                    result.setT_integer(Integer.parseInt(array[<span class="number">1</span>]));</span><br><span class="line">                    result.setT_boolean(<span class="literal">true</span>);</span><br><span class="line">                    result.setT_double(<span class="number">12.12</span>);</span><br><span class="line">                    Decimal t_decimal = <span class="keyword">new</span> Decimal();</span><br><span class="line">                    t_decimal.set(<span class="keyword">new</span> scala.math.BigDecimal(<span class="keyword">new</span> BigDecimal(<span class="string">"11111111.11111111"</span>)));</span><br><span class="line">                    result.setT_decimal(t_decimal);</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 不能使用hiveContext.createDataFrame(map, TestBean.class);</span></span><br><span class="line">                    <span class="comment">// 字段顺序不一致，不知道是bug还是什么。所以只能选择row</span></span><br><span class="line">                    <span class="keyword">return</span> result.toRow();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Generate the schema based on the string of schema</span></span><br><span class="line">            List&lt;StructField&gt; fields = <span class="keyword">new</span> ArrayList&lt;StructField&gt;();</span><br><span class="line">            fields.add(DataTypes.createStructField(<span class="string">"t_string"</span>, DataTypes.StringType, <span class="literal">true</span>));</span><br><span class="line">            fields.add(DataTypes.createStructField(<span class="string">"t_integer"</span>, DataTypes.IntegerType, <span class="literal">true</span>));</span><br><span class="line">            fields.add(DataTypes.createStructField(<span class="string">"t_boolean"</span>, DataTypes.BooleanType, <span class="literal">true</span>));</span><br><span class="line">            fields.add(DataTypes.createStructField(<span class="string">"t_double"</span>, DataTypes.DoubleType, <span class="literal">true</span>));</span><br><span class="line">            fields.add(DataTypes.createStructField(<span class="string">"t_decimal"</span>, DataTypes.createDecimalType(<span class="number">20</span>, <span class="number">8</span>), <span class="literal">true</span>));</span><br><span class="line"></span><br><span class="line">            StructType schema = DataTypes.createStructType(fields);</span><br><span class="line"></span><br><span class="line">            DataFrame df = hiveContext.createDataFrame(map, schema);</span><br><span class="line">            df.write().format(<span class="string">"orc"</span>).mode(SaveMode.Append).saveAsTable(<span class="string">"test.test"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>TestBean.java：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Row;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.RowFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.Decimal;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestBean</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * </span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">-5868257956951746438</span>L;</span><br><span class="line">    <span class="keyword">private</span> String t_string;</span><br><span class="line">    <span class="keyword">private</span> Integer t_integer;</span><br><span class="line">    <span class="keyword">private</span> Boolean t_boolean;</span><br><span class="line">    <span class="keyword">private</span> Double t_double;</span><br><span class="line">    <span class="keyword">private</span> Decimal t_decimal;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="function">String <span class="title">getT_string</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> t_string;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">setT_string</span><span class="params">(String t_string)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.t_string = t_string;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="function">Integer <span class="title">getT_integer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> t_integer;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">setT_integer</span><span class="params">(Integer t_integer)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.t_integer = t_integer;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="function">Boolean <span class="title">getT_boolean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> t_boolean;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">setT_boolean</span><span class="params">(Boolean t_boolean)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.t_boolean = t_boolean;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="function">Double <span class="title">getT_double</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> t_double;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">setT_double</span><span class="params">(Double t_double)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.t_double = t_double;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="function">Decimal <span class="title">getT_decimal</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> t_decimal;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">setT_decimal</span><span class="params">(Decimal t_decimal)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.t_decimal = t_decimal;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="function">Row <span class="title">toRow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">return</span> RowFactory.<span class="title">create</span><span class="params">(t_string, t_integer, t_boolean, t_double, t_decimal)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>根据以上代码，编译打包即刻使用spark-submit命令执行：</p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --<span class="keyword">master</span> <span class="title">yarn-client</span> --class Demo xx.jar</span><br></pre></td></tr></table></figure>

<p>数据查询：<br><br><img src="http://www.zhangrenhua.com/back_images/spark-hive-insert-query.png" alt="Alt text"></p>
<h3 id="要点记录"><a href="#要点记录" class="headerlink" title="要点记录"></a>要点记录</h3><p>1、根据官方例子中使用hiveContext.createDataFrame(map, TestBean.class);是可以将对象转换成DataFrame的，<font color="red">但是我在测试中发现如果使用这种方式创建DataFrame，最终存到hive表中的数据字段是不对的。</font>，所以还是自定义schema，返回Row对象。</p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">RowFactory.create(t_string, t_integer, t_boolean, t_double, t_decimal);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Generate the schema based on the string of schema</span></span><br><span class="line">List&lt;StructField&gt; fields = <span class="keyword">new</span> ArrayList&lt;StructField&gt;();</span><br><span class="line">fields.<span class="keyword">add</span>(DataTypes.createStructField(<span class="string">"t_string"</span>, DataTypes.StringType, <span class="literal">true</span>));</span><br><span class="line">fields.<span class="keyword">add</span>(DataTypes.createStructField(<span class="string">"t_integer"</span>, DataTypes.IntegerType, <span class="literal">true</span>));</span><br><span class="line">fields.<span class="keyword">add</span>(DataTypes.createStructField(<span class="string">"t_boolean"</span>, DataTypes.BooleanType, <span class="literal">true</span>));</span><br><span class="line">fields.<span class="keyword">add</span>(DataTypes.createStructField(<span class="string">"t_double"</span>, DataTypes.DoubleType, <span class="literal">true</span>));</span><br><span class="line">fields.<span class="keyword">add</span>(DataTypes.createStructField(<span class="string">"t_decimal"</span>, DataTypes.createDecimalType(<span class="number">20</span>, <span class="number">8</span>), <span class="literal">true</span>));</span><br></pre></td></tr></table></figure>

<p><font color="red">创建List<structfield> fields和Row对象时，传入的字段顺序一定要和表中的顺序保持一致。</structfield></font></p>
<p>2、将DataFrame存入自定义数据库的表中</p>
<figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.saveAsTable(<span class="string">"test.test"</span>)<span class="comment">;</span></span><br></pre></td></tr></table></figure>

<p>从spark1.5.1开始支持显示指定数据库名，在spark1.5.1之前的版本需要使用下面方法：</p>
<figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hiveContext.sql(<span class="string">"use test"</span>)<span class="comment">;</span></span><br></pre></td></tr></table></figure>

<h2 id="DataFrame-Save-As-分区表"><a href="#DataFrame-Save-As-分区表" class="headerlink" title="DataFrame Save As 分区表"></a>DataFrame Save As 分区表</h2><p>上面演示了如何将常用数据类型（string、integer、boolean、double、decimal）的DataFrame存入到hive表中。但是没有描述如何将数据存入<font color="red">分区表</font>中，下面我会给出如何写入分区表中的具体逻辑。</p>
<p>1、动态分区</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置动态分区</span></span><br><span class="line"><span class="selector-tag">hiveContext</span><span class="selector-class">.sql</span>(<span class="string">"set hive.exec.dynamic.partition=true"</span>);</span><br><span class="line"><span class="selector-tag">hiveContext</span><span class="selector-class">.sql</span>(<span class="string">"set hive.exec.dynamic.partition.mode=nonstrict"</span>);</span><br></pre></td></tr></table></figure>

<p>创建Row对象时，<strong>将分区字段放到最后</strong>：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">RowFactory</span><span class="selector-class">.create</span>(t_string, t_integer, t_boolean, t_double, t_decimal, <span class="string">'分区字段'</span>);</span><br></pre></td></tr></table></figure>

<p>df.saveAsTable(“test.test”);</p>
<p>使用动态分区会影响写入性能，如果分区字段是可以固定的，则建议使用下面 <strong>固定分区</strong>方法。</p>
<p>2、固定分区</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.registerTempTable("demo");</span><br><span class="line">hiveContext.sql("<span class="keyword">insert</span> <span class="keyword">into</span> test.test <span class="keyword">partition</span>(<span class="built_in">date</span>=<span class="string">'20151205'</span>) <span class="keyword">select</span> * <span class="keyword">from</span> demo<span class="string">");</span></span><br></pre></td></tr></table></figure>

<p>注：date为分区字段，20151205为分区值，<strong>可由参数传入</strong>。</p>
<h2 id="Metadata-Refreshing"><a href="#Metadata-Refreshing" class="headerlink" title="Metadata Refreshing"></a>Metadata Refreshing</h2><p>Spark SQL caches Parquet metadata for better performance. When Hive metastore Parquet table conversion is enabled, metadata of those converted tables are also cached. If these tables are updated by Hive or other external tools, you need to refresh them manually to ensure consistent metadata.</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sqlContext is an existing HiveContext</span></span><br><span class="line"><span class="selector-tag">sqlContext</span><span class="selector-class">.refreshTable</span>(<span class="string">"my_table"</span>)</span><br></pre></td></tr></table></figure>

<h2 id="DataFrame-Operations"><a href="#DataFrame-Operations" class="headerlink" title="DataFrame Operations"></a>DataFrame Operations</h2><p>DataFrame提供一个结构化的数据，下面是使用Java操作DataFrame的一些基本例子：</p>
<figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">JavaSparkContext sc <span class="comment">// An existing SparkContext.</span></span><br><span class="line">SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create the DataFrame</span></span><br><span class="line">DataFrame df = sqlContext.read().json(<span class="string">"examples/src/main/resources/people.json"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Show the content of the DataFrame</span></span><br><span class="line">df.<span class="built_in">show</span>();</span><br><span class="line"><span class="comment">// age  name</span></span><br><span class="line"><span class="comment">// null Michael</span></span><br><span class="line"><span class="comment">// 30   Andy</span></span><br><span class="line"><span class="comment">// 19   Justin</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Print the schema in a tree format</span></span><br><span class="line">df.printSchema();</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">// |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment">// |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select only the "name" column</span></span><br><span class="line">df.<span class="keyword">select</span>(<span class="string">"name"</span>).<span class="built_in">show</span>();</span><br><span class="line"><span class="comment">// name</span></span><br><span class="line"><span class="comment">// Michael</span></span><br><span class="line"><span class="comment">// Andy</span></span><br><span class="line"><span class="comment">// Justin</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select everybody, but increment the age by 1</span></span><br><span class="line">df.<span class="keyword">select</span>(df.col(<span class="string">"name"</span>), df.col(<span class="string">"age"</span>).plus(<span class="number">1</span>)).<span class="built_in">show</span>();</span><br><span class="line"><span class="comment">// name    (age + 1)</span></span><br><span class="line"><span class="comment">// Michael null</span></span><br><span class="line"><span class="comment">// Andy    31</span></span><br><span class="line"><span class="comment">// Justin  20</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Select people older than 21</span></span><br><span class="line">df.filter(df.col(<span class="string">"age"</span>).gt(<span class="number">21</span>)).<span class="built_in">show</span>();</span><br><span class="line"><span class="comment">// age name</span></span><br><span class="line"><span class="comment">// 30  Andy</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Count people by age</span></span><br><span class="line">df.groupBy(<span class="string">"age"</span>).<span class="built_in">count</span>().<span class="built_in">show</span>();</span><br><span class="line"><span class="comment">// age  count</span></span><br><span class="line"><span class="comment">// null 1</span></span><br><span class="line"><span class="comment">// 19   1</span></span><br><span class="line"><span class="comment">// 30   1</span></span><br></pre></td></tr></table></figure>

<h2 id="Parquet-Configuration"><a href="#Parquet-Configuration" class="headerlink" title="Parquet Configuration"></a>Parquet Configuration</h2><p>Configuration of Parquet can be done using the <code>setConf</code> method on <code>SQLContext</code> or by running<br><code>SET key=value</code> commands using SQL.</p>
<table class="table">
<tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr>
<tr>
  <td><code>spark.sql.parquet.binaryAsString</code></td>
  <td>false</td>
  <td>
    Some other Parquet-producing systems, in particular Impala, Hive, and older versions of Spark SQL, do
    not differentiate between binary data and strings when writing out the Parquet schema. This
    flag tells Spark SQL to interpret binary data as a string to provide compatibility with these systems.
  </td>
</tr>
<tr>
  <td><code>spark.sql.parquet.int96AsTimestamp</code></td>
  <td>true</td>
  <td>
    Some Parquet-producing systems, in particular Impala and Hive, store Timestamp into INT96.  This
    flag tells Spark SQL to interpret INT96 data as a timestamp to provide compatibility with these systems.
  </td>
</tr>
<tr>
  <td><code>spark.sql.parquet.cacheMetadata</code></td>
  <td>true</td>
  <td>
    Turns on caching of Parquet schema metadata. Can speed up querying of static data.
  </td>
</tr>
<tr>
  <td><code>spark.sql.parquet.compression.codec</code></td>
  <td>gzip</td>
  <td>
    Sets the compression codec use when writing Parquet files. Acceptable values include:
    uncompressed, snappy, gzip, lzo.
  </td>
</tr>
<tr>
  <td><code>spark.sql.parquet.filterPushdown</code></td>
  <td>true</td>
  <td>Enables Parquet filter push-down optimization when set to true.</td>
</tr>
<tr>
  <td><code>spark.sql.hive.convertMetastoreParquet</code></td>
  <td>true</td>
  <td>
    When set to false, Spark SQL will use the Hive SerDe for parquet tables instead of the built in
    support.
  </td>
</tr>
<tr>
  <td><code>spark.sql.parquet.output.committer.class</code></td>
  <td><code>org.apache.parquet.hadoop.<br>ParquetOutputCommitter</code></td>
  <td>
    <p>
      The output committer class used by Parquet. The specified class needs to be a subclass of
      <code>org.apache.hadoop.<br>mapreduce.OutputCommitter</code>.  Typically, it's also a
      subclass of <code>org.apache.parquet.hadoop.ParquetOutputCommitter</code>.
    </p>
    <p>
      <b>Note:</b>
      <ul>
        <li>
          This option is automatically ignored if <code>spark.speculation</code> is turned on.
        </li>
        <li>
          This option must be set via Hadoop <code>Configuration</code> rather than Spark
          <code>SQLConf</code>.
        </li>
        <li>
          This option overrides <code>spark.sql.sources.<br>outputCommitterClass</code>.
        </li>
      </ul>
    </p>
    <p>
      Spark SQL comes with a builtin
      <code>org.apache.spark.sql.<br>parquet.DirectParquetOutputCommitter</code>, which can be more
      efficient then the default Parquet output committer when writing data to S3.
    </p>
  </td>
</tr>
<tr>
  <td><code>spark.sql.parquet.mergeSchema</code></td>
  <td><code>false</code></td>
  <td>
    <p>
      When true, the Parquet data source merges schemas collected from all data files, otherwise the
      schema is picked from the summary file or a random data file if no summary file is available.
    </p>
  </td>
</tr>
</table>

<h2 id="Interacting-with-Different-Versions-of-Hive-Metastore"><a href="#Interacting-with-Different-Versions-of-Hive-Metastore" class="headerlink" title="Interacting with Different Versions of Hive Metastore"></a>Interacting with Different Versions of Hive Metastore</h2><p>One of the most important pieces of Spark SQL’s Hive support is interaction with Hive metastore, which enables Spark SQL to access metadata of Hive tables. Starting from Spark 1.4.0, a single binary build of Spark SQL can be used to query different versions of Hive metastores, using the configuration described below. Note that independent of the version of Hive that is being used to talk to the metastore, internally Spark SQL will compile against Hive 1.2.1 and use those classes for internal execution (serdes, UDFs, UDAFs, etc).</p>
<p>The following options can be used to configure the version of Hive that is used to retrieve metadata:</p>
<table class="table">
  <tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr>
  <tr>
    <td><code>spark.sql.hive.metastore.version</code></td>
    <td><code>1.2.1</code></td>
    <td>
      Version of the Hive metastore. Available
      options are <code>0.12.0</code> through <code>1.2.1</code>.
    </td>
  </tr>
  <tr>
    <td><code>spark.sql.hive.metastore.jars</code></td>
    <td><code>builtin</code></td>
    <td>
      Location of the jars that should be used to instantiate the HiveMetastoreClient. This
      property can be one of three options:
      <ol>
        <li><code>builtin</code></li>
        Use Hive 1.2.1, which is bundled with the Spark assembly jar when <code>-Phive</code> is
        enabled. When this option is chosen, <code>spark.sql.hive.metastore.version</code> must be
        either <code>1.2.1</code> or not defined.
        <li><code>maven</code></li>
        Use Hive jars of specified version downloaded from Maven repositories.  This configuration
        is not generally recommended for production deployments. 
        <li>A classpath in the standard format for the JVM.  This classpath must include all of Hive 
        and its dependencies, including the correct version of Hadoop.  These jars only need to be
        present on the driver, but if you are running in yarn cluster mode then you must ensure
        they are packaged with you application.</li>
      </ol>
    </td>
  </tr>
  <tr>
    <td><code>spark.sql.hive.metastore.sharedPrefixes</code></td>
    <td><code>com.mysql.jdbc,<br>org.postgresql,<br>com.microsoft.sqlserver,<br>oracle.jdbc</code></td>
    <td>
      <p>
        A comma separated list of class prefixes that should be loaded using the classloader that is
        shared between Spark SQL and a specific version of Hive. An example of classes that should
        be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need
        to be shared are those that interact with classes that are already shared. For example,
        custom appenders that are used by log4j.
      </p>
    </td>
  </tr>
  <tr>
    <td><code>spark.sql.hive.metastore.barrierPrefixes</code></td>
    <td><code>(empty)</code></td>
    <td>
      <p>
        A comma separated list of class prefixes that should explicitly be reloaded for each version
        of Hive that Spark SQL is communicating with. For example, Hive UDFs that are declared in a
        prefix that typically would be shared (i.e. <code>org.apache.spark.*</code>).
      </p>
    </td>
  </tr>
</table>

<h2 id="JDBC-To-Other-Databases"><a href="#JDBC-To-Other-Databases" class="headerlink" title="JDBC To Other Databases"></a>JDBC To Other Databases</h2><p>Spark SQL also includes a data source that can read data from other databases using JDBC. This functionality should be preferred over using JdbcRDD. This is because the results are returned as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources. The JDBC data source is also easier to use from Java or Python as it does not require the user to provide a ClassTag. (Note that this is different than the Spark SQL JDBC server, which allows other applications to run queries using Spark SQL).</p>
<p>To get started you will need to include the JDBC driver for you particular database on the spark classpath. For example, to connect to postgres from the Spark Shell you would run the following command:</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_CLASSPATH=postgresql-<span class="number">9.3</span>-<span class="number">1102</span>-jdbc41.jar bin/spark-<span class="keyword">shell</span><span class="bash"></span></span><br></pre></td></tr></table></figure>

<p>Tables from the remote database can be loaded as a DataFrame or Spark SQL Temporary table using the Data Sources API. The following options are supported:</p>
<table class="table">
  <tr><th>Property Name</th><th>Meaning</th></tr>
  <tr>
    <td><code>url</code></td>
    <td>
      The JDBC URL to connect to.
    </td>
  </tr>
  <tr>
    <td><code>dbtable</code></td>
    <td>
      The JDBC table that should be read.  Note that anything that is valid in a <code>FROM</code> clause of
      a SQL query can be used.  For example, instead of a full table you could also use a
      subquery in parentheses.
    </td>
  </tr>

  <tr>
    <td><code>driver</code></td>
    <td>
      The class name of the JDBC driver needed to connect to this URL.  This class will be loaded
      on the master and workers before running an JDBC commands to allow the driver to
      register itself with the JDBC subsystem.
    </td>
  </tr>
  <tr>
    <td><code>partitionColumn, lowerBound, upperBound, numPartitions</code></td>
    <td>
      These options must all be specified if any of them is specified.  They describe how to
      partition the table when reading in parallel from multiple workers.
      <code>partitionColumn</code> must be a numeric column from the table in question. Notice
      that <code>lowerBound</code> and <code>upperBound</code> are just used to decide the
      partition stride, not for filtering the rows in table. So all rows in the table will be
      partitioned and returned.
    </td>
  </tr>
</table>

<p>示例：</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; options = <span class="keyword">new</span> HashMap&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;();</span><br><span class="line">options.<span class="built_in">put</span>(<span class="string">"url"</span>, <span class="string">"jdbc:postgresql:dbserver"</span>);</span><br><span class="line">options.<span class="built_in">put</span>(<span class="string">"dbtable"</span>, <span class="string">"schema.tablename"</span>);</span><br><span class="line"></span><br><span class="line">DataFrame jdbcDF = sqlContext.<span class="built_in">read</span>().format(<span class="string">"jdbc"</span>). options(options).load();</span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#interacting-with-different-versions-of-hive-metastore" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-programming-guide.html#interacting-with-different-versions-of-hive-metastore</a></li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/java/" rel="tag"># java</a>
          
            <a href="/tags/hadoop生态圈/" rel="tag"># hadoop生态圈</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2015/11/27/hadoop-mapreduce执行流程分析/" rel="next" title="MapReduce执行流程分析">
                <i class="fa fa-chevron-left"></i> MapReduce执行流程分析
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2015/12/05/hadoop-eclipse开发环境搭建/" rel="prev" title="hadoop2.x-Eclipse开发环境搭建">
                hadoop2.x-Eclipse开发环境搭建 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
<span class="jiathis_txt">分享到：</span>
<a class="jiathis_button_fav">收藏夹</a>
<a class="jiathis_button_copy">复制网址</a>
<a class="jiathis_button_email">邮件</a>
<a class="jiathis_button_weixin">微信</a>
<a class="jiathis_button_qzone">QQ空间</a>
<a class="jiathis_button_tqq">腾讯微博</a>
<a class="jiathis_button_douban">豆瓣</a>
<a class="jiathis_button_share">一键分享</a>

<a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript">
var jiathis_config={
  data_track_clickback:true,
  summary:"",
  shortUrl:false,
  hideMore:false
}
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=" charset="utf-8"></script>
<!-- JiaThis Button END -->
      
    </div>
  </div>


          </div>
          

  <p>热评文章</p>
  <div class="ds-top-threads" data-range="weekly" data-num-items="4"></div>


          

  
    <div class="comments" id="comments">
      <div class="ds-thread" data-thread-key="2015/11/28/hadoop-spark-dataFrame操作hive/" data-title="Spark DataFrme操作Hive" data-url="http://www.zhangrenhua.com/2015/11/28/hadoop-spark-dataFrame操作hive/">
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="http://www.zhangrenhua.com/back_images/avatar.jpg" alt="张仁华">
            
              <p class="site-author-name" itemprop="name">张仁华</p>
              <p class="site-description motion-element" itemprop="description">记录软件生涯中的点点滴滴</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">76</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/zhangrenhua/" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="http://weibo.com/huaHBrother" target="_blank" title="Weibo">
                      
                        <i class="fa fa-fw fa-globe"></i>Weibo</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="http://douban.com/people/huaHBrother" target="_blank" title="douban">
                      
                        <i class="fa fa-fw fa-globe"></i>douban</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="http://zhihu.com/people/huaHBrother" target="_blank" title="zhihu">
                      
                        <i class="fa fa-fw fa-globe"></i>zhihu</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#背景"><span class="nav-number">1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrame-SaveAsTable"><span class="nav-number">2.</span> <span class="nav-text">DataFrame SaveAsTable</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#示例"><span class="nav-number">2.1.</span> <span class="nav-text">示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#要点记录"><span class="nav-number">2.2.</span> <span class="nav-text">要点记录</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrame-Save-As-分区表"><span class="nav-number">3.</span> <span class="nav-text">DataFrame Save As 分区表</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Metadata-Refreshing"><span class="nav-number">4.</span> <span class="nav-text">Metadata Refreshing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrame-Operations"><span class="nav-number">5.</span> <span class="nav-text">DataFrame Operations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parquet-Configuration"><span class="nav-number">6.</span> <span class="nav-text">Parquet Configuration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Interacting-with-Different-Versions-of-Hive-Metastore"><span class="nav-number">7.</span> <span class="nav-text">Interacting with Different Versions of Hive Metastore</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JDBC-To-Other-Databases"><span class="nav-number">8.</span> <span class="nav-text">JDBC To Other Databases</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">9.</span> <span class="nav-text">参考资料</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张仁华</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"iissnan-notes"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  


















  





  

  

  

  
  

  

  

  

</body>
</html>
